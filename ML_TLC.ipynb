{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX9JsZYCTRdp7xW62Jxrrg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0ethel0zhang/quantcon2022/blob/wip/ML_TLC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTuvinUkc4Jq"
      },
      "source": [
        "# Goal\n",
        "Coding exercise for [Representing Voices of Users at Scale, ML TLC for thematic coding](https://docs.google.com/presentation/d/13YIPU32a_HFGwlszvdMIVP9TSCd9GNoRYsviZanm-yc/edit?resourcekey=0-fdt0Kj8mVLrFdBNbRuxTLw#slide=id.g14d94fc545e_0_57) (code self link: go/ml_tlc).\n",
        "\n",
        "You can follow along the quick 15-min walk-through video [here](https://www.google.com/url?q=https://drive.google.com/file/d/1mi9vxcRbpwtpV-XJ1ug68_8nnezurXoL/view?usp%3Dshare_link%26resourcekey%3D0-lMaxNmLZZLxHmZHRN4cf2A&sa=D&source=editors&ust=1675782918303743&usg=AOvVaw1yRLx1vglBZ2NrwUswLcpg)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H80ZU8lM64qT"
      },
      "source": [
        "# 0. Setup\n",
        "\n",
        "**ATTENTION:** To use the Tensorflow Hub in this colab, you need to connect to the TF-enabled runtime. <br/>\n",
        "If your local runtime doesn't support tensorflow, you can use a Brain Frameworks runtime. The details of running it can be found [here](https://docs.google.com/document/d/1sCvpCNXq4hOpDWKEcfC5rie0f7JTpqzwVhLKqiLlZZ8/edit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekA43RBgH0fV"
      },
      "source": [
        "## 0.1 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI5kHrJlyAro"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from colabtools import sheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkPfDeey2OEc"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.enable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60l3vW2d8N31"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import string\n",
        "\n",
        "import google3\n",
        "\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import nltk.data\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hOEQZerxSjLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZZxaeiJHEgn"
      },
      "outputs": [],
      "source": [
        "from google.colab import data_table\n",
        "# more info https://colab.sandbox.google.com/notebooks/data_table.ipynb\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDLbmdwMdNP-"
      },
      "source": [
        "# 1. Acquire the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9vXypnW60hY"
      },
      "source": [
        "##1.1 Download CLINC150\n",
        "\n",
        "CLINC150 is a new dataset that includes queries with intents and queries that are out-of-scope (OOS), i.e., queries that do not fall into any of the system's supported intents.\n",
        "\n",
        "The CLINC150 dataset is available on [TensorFlow datasets](https://github.com/tensorflow/datasets). The following code downloads the CLINC150 dataset to your machine (or the colab runtime):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIzHt5486hEu"
      },
      "outputs": [],
      "source": [
        "data = tfds.load(name = \"clinc_oos\", split = \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szGPLd0y6sBL"
      },
      "outputs": [],
      "source": [
        "sample_n = 500 #@param sample_n\n",
        "text = [tfds.as_numpy(x[\"text\"]) for x in data.take(sample_n)]\n",
        "intent = [tfds.as_numpy(x[\"intent_name\"]) for x in data.take(sample_n)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "source": [
        "## 1.2 Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "outputs": [],
      "source": [
        "print(\"Text entries: {}, intent entries: {}\".format(len(text), len(intent)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "source": [
        "Let's look at some of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ila0FuV6MMK2"
      },
      "outputs": [],
      "source": [
        "text[-3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "# What's next? To build the model\n",
        "\n",
        "Building a clustering model requires three main architectural decisions:\n",
        "\n",
        "* How to represent the text?\n",
        "* Which clustering model to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dleWkTmqIg1l"
      },
      "source": [
        "# 2. Transform the responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTVTYuzuQQ9M"
      },
      "source": [
        "## 2.1 Transform input into embeddings\n",
        "\n",
        "In this example, the input data consists of sentences. As suggested by TensorFlow, \"One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have two advantages:\n",
        "*   we don't have to worry about text preprocessing,\n",
        "*   we can benefit from transfer learning.\n",
        "\"\n",
        "\n",
        "\n",
        "Here are many [embeddings models](https://www.tensorflow.org/text/guide/word_embeddings) from [TensorFlow Hub](https://www.tensorflow.org/hub) that you can try. This turorial will use the Universal Language Encoder i.e. **Option 1** in the following list of light-weight embeddings:\n",
        "\n",
        "* [universal-sentence-encoder](https://tfhub.dev/google/universal-sentence-encoder/4) \"*The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.*\"\n",
        "* [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2) - 50-dimension embedding trained on English Google News 7B corpus\n",
        "* [google/nnlm-en-dim50-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2) - same as [google/nnlm-en-dim50/2](https://tfhub.dev/google/nnlm-en-dim50/2), but with additional text normalization to remove punctuation. This can help to get better coverage of in-vocabulary embeddings for tokens on your input text.\n",
        "* [google/nnlm-en-dim128-with-normalization/2](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) - A larger model with an embedding dimension of 128 instead of  50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kLIgt0Z0RBU"
      },
      "outputs": [],
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "kona2 = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9eVYTebRdgD"
      },
      "outputs": [],
      "source": [
        "def plot_similarity(labels, features, rotation1, rotation2):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation1)\n",
        "  g.set_yticklabels(labels, rotation=rotation2)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, message_embeddings_, 0, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC86r21Roq9H"
      },
      "source": [
        "## 2.2 Clean text\n",
        "\n",
        "### Data Cleaning: This example follows the following data cleaning pipeline:\n",
        "- Lower case\n",
        "- Expand contractions\n",
        "- Remove punctuations\n",
        "- Word Tokenization\n",
        "- POS tagging\n",
        "- Lemmatization (you can also choose stemming)\n",
        "- [Optional] Remove verbs\n",
        "- Remove stop words\n",
        "\n",
        "\n",
        "More details can be found in this [slide deck](https://docs.google.com/presentation/d/1WUflQdOTCr9SDWcyHKmXYPNmTZ2OvGiljicD5CS3gV8/edit#slide=id.ge7c4bcb690_0_0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a1h7RIgTox5P"
      },
      "outputs": [],
      "source": [
        "#@title Import NLTK library [CODE BLOCK]\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import ngrams\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# First run must be inside the adhoc_import context to enable nltk to find its data files.\n",
        "nltk.ne_chunk(nltk.pos_tag(word_tokenize('warmup')), binary=True)\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "print(wordnet_lemmatizer.lemmatize('dogs'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import personalized cleaning functions\n",
        "class text_cleaning_helper():\n",
        "  # The pipeline of text preprocessing does the following steps on each sentence\n",
        "  #   1. Convert text to lowercase\n",
        "  #   2. Expand contractions\n",
        "  #   3. Remove punctuations while preserving google internal links (e.g. go/)\n",
        "  #   4. Lemmatize with the option to keep certain POS only\n",
        "  #   5. Remove stop words\n",
        "\n",
        "  def __init__(self):\n",
        "    self.status = True\n",
        "    # compile all punctuations besides _ and / (for links)\n",
        "    self.punc = string.punctuation.replace(\"_\", \"\").replace(\"/\", \"\")\n",
        "    # create English stop words list (you can always define your own stopwords)\n",
        "    self.stop_words = set(stopwords.words(\"english\"))\n",
        "    # add to stop words\n",
        "    for x in [\n",
        "        \"ive\", \"could\", \"would\", \"also\", \"need\", \"want\", \"also\", \"though\",\n",
        "        \"even\", \"etc\", \"within\"\n",
        "    ]:\n",
        "      self.stop_words.add(x)\n",
        "    self.wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    # set up [contractions](https://source.corp.google.com/piper///depot/google3/java/com/google/corp/culture/analytics/analysis/servlets/textnormalization/Contractions.java)\n",
        "    self.contractions = {\n",
        "      \"he's\": \"he is\",\n",
        "      \"i'll\": \"i will\",\n",
        "      \"i've\": \"i have\",\n",
        "      \"it'd\": \"it had\",\n",
        "      \"it's\": \"it is\",\n",
        "      \"so's\": \"so is\",\n",
        "      \"we'd\": \"we had\",\n",
        "      \"i'd\": \"i would\",\n",
        "      \"i'm\": \"i am\",\n",
        "      \"shouldn't've\": \"should not have\",\n",
        "      \"couldn't've\": \"could not have\",\n",
        "      \"mightn't've\": \"might not have\",\n",
        "      \"oughtn't've\": \"ought not have\",\n",
        "      \"wouldn't've\": \"would not have\",\n",
        "      \"mustn't've\": \"must not have\",\n",
        "      \"needn't've\": \"need not have\",\n",
        "      \"there'd've\": \"there would have\",\n",
        "      \"they'll've\": \"they will have\",\n",
        "      \"what'll've\": \"what will have\",\n",
        "      \"y'all'd've\": \"you all would have\",\n",
        "      \"hadn't've\": \"had not have\",\n",
        "      \"shan't've\": \"shall not have\",\n",
        "      \"she'll've\": \"she will have\",\n",
        "      \"should've\": \"should have\",\n",
        "      \"shouldn't\": \"should not\",\n",
        "      \"that'd've\": \"that would have\",\n",
        "      \"they'd've\": \"they would have\",\n",
        "      \"who'll've\": \"who will have\",\n",
        "      \"you'll've\": \"you will have\",\n",
        "      \"can't've\": \"cannot have\",\n",
        "      \"could've\": \"could have\",\n",
        "      \"couldn't\": \"could not\",\n",
        "      \"he'll've\": \"he will have\",\n",
        "      \"it'll've\": \"it will have\",\n",
        "      \"might've\": \"might have\",\n",
        "      \"mightn't\": \"might not\",\n",
        "      \"oughtn't\": \"ought not\",\n",
        "      \"she'd've\": \"she would have\",\n",
        "      \"we'll've\": \"we will have\",\n",
        "      \"where've\": \"where have\",\n",
        "      \"won't've\": \"will not have\",\n",
        "      \"would've\": \"would have\",\n",
        "      \"wouldn't\": \"would not\",\n",
        "      \"y'all're\": \"you all are\",\n",
        "      \"y'all've\": \"you all have\",\n",
        "      \"you'd've\": \"you would have\",\n",
        "      \"doesn't\": \"does not\",\n",
        "      \"haven't\": \"have not\",\n",
        "      \"he'd've\": \"he would have\",\n",
        "      \"how'd'y\": \"how do you\",\n",
        "      \"i'll've\": \"i will have\",\n",
        "      \"it'd've\": \"it would have\",\n",
        "      \"must've\": \"must have\",\n",
        "      \"mustn't\": \"must not\",\n",
        "      \"needn't\": \"need not\",\n",
        "      \"o'clock\": \"of the clock\",\n",
        "      \"sha'n't\": \"shall not\",\n",
        "      \"there'd\": \"there had\",\n",
        "      \"there's\": \"there is\",\n",
        "      \"they'll\": \"they will\",\n",
        "      \"they're\": \"they are\",\n",
        "      \"they've\": \"they have\",\n",
        "      \"we'd've\": \"we would have\",\n",
        "      \"weren't\": \"were not\",\n",
        "      \"what'll\": \"what will\",\n",
        "      \"what're\": \"what are\",\n",
        "      \"what've\": \"what have\",\n",
        "      \"when've\": \"when have\",\n",
        "      \"where'd\": \"where did\",\n",
        "      \"where's\": \"where is\",\n",
        "      \"will've\": \"will have\",\n",
        "      \"y'all'd\": \"you all would\",\n",
        "      \"aren't\": \"are not\",\n",
        "      \"'cause\": \"because\",\n",
        "      \"didn't\": \"did not\",\n",
        "      \"hadn't\": \"had not\",\n",
        "      \"hasn't\": \"has not\",\n",
        "      \"how'll\": \"how will\",\n",
        "      \"i'd've\": \"i would have\",\n",
        "      \"mayn't\": \"may not\",\n",
        "      \"musn't\": \"must not\",\n",
        "      \"shan't\": \"shall not\",\n",
        "      \"she'll\": \"she will\",\n",
        "      \"that'd\": \"that would\",\n",
        "      \"that's\": \"that is\",\n",
        "      \"they'd\": \"they would\",\n",
        "      \"wasn't\": \"was not\",\n",
        "      \"what's\": \"what is\",\n",
        "      \"when's\": \"when is\",\n",
        "      \"who'll\": \"who will\",\n",
        "      \"who've\": \"who have\",\n",
        "      \"why've\": \"why have\",\n",
        "      \"y'alls\": \"you alls\",\n",
        "      \"you'll\": \"you will\",\n",
        "      \"you're\": \"you are\",\n",
        "      \"you've\": \"you have\",\n",
        "      \"ain't\": \"am not\",\n",
        "      \"can't\": \"cannot\",\n",
        "      \"don't\": \"do not\",\n",
        "      \"he'll\": \"he will\",\n",
        "      \"how'd\": \"how did\",\n",
        "      \"how's\": \"how is\",\n",
        "      \"isn't\": \"is not\",\n",
        "      \"it'll\": \"it will\",\n",
        "      \"let's\": \"let us\",\n",
        "      \"ma'am\": \"madam\",\n",
        "      \"she'd\": \"she would\",\n",
        "      \"she's\": \"she is\",\n",
        "      \"so've\": \"so have\",\n",
        "      \"to've\": \"to have\",\n",
        "      \"we'll\": \"we will\",\n",
        "      \"we're\": \"we are\",\n",
        "      \"we've\": \"we have\",\n",
        "      \"who's\": \"who is\",\n",
        "      \"why's\": \"why is\",\n",
        "      \"won't\": \"will not\",\n",
        "      \"y'all\": \"you all\",\n",
        "      \"you'd\": \"you had\",\n",
        "      \"he'd\": \"he would\",\n",
        "      \"he's\": \"he is\",\n",
        "      \"i'll\": \"i will\",\n",
        "      \"i've\": \"i have\",\n",
        "      \"it'd\": \"it had\",\n",
        "      \"it's\": \"it is\",\n",
        "      \"so's\": \"so is\",\n",
        "      \"we'd\": \"we had\",\n",
        "      \"i'd\": \"i would\",\n",
        "      \"i'm\": \"i am\"\n",
        "    }\n",
        "\n",
        "  def remove_multiple_spaces(self, word_string):\n",
        "    \"\"\"Remove multiple spaces and return string.\n",
        "\n",
        "     Args:\n",
        "       word_string: a str object of the text to be cleaned\n",
        "     Returns:\n",
        "       a cleaned string\n",
        "    \"\"\"\n",
        "    found = re.findall(\"\\s+\", word_string)\n",
        "    for f in set(found):\n",
        "      word_string = word_string.replace(f, \" \")\n",
        "    return word_string\n",
        "\n",
        "  def remove_multiple_underscores(self, word_string):\n",
        "    \"\"\"Remove multiple underscores and return string.\n",
        "\n",
        "    Args:\n",
        "      word_string: a str object of the text to be cleaned\n",
        "    Returns:\n",
        "      a cleaned string\n",
        "    \"\"\"\n",
        "    found = re.findall(\"_{2,}\", word_string)\n",
        "    for f in set(found):\n",
        "      word_string = word_string.replace(f, \"_\")\n",
        "    return word_string\n",
        "\n",
        "  def combined_processor(self, word_string):\n",
        "    \"\"\"Combine multiple text cleaning functions to one.\n",
        "\n",
        "    Args:\n",
        "      word_string: a str object of the text to be cleaned\n",
        "    Returns:\n",
        "      cleaned string.\n",
        "    \"\"\"\n",
        "    return self.remove_multiple_spaces(\n",
        "        self.remove_multiple_underscores(\n",
        "            word_string.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\n",
        "                \"â\", \"'\").replace(\"Â\\xa0\", \"\").replace(\"â\", \"\").replace(\n",
        "                    \"Â\", \"\").replace(\"as well as\",\n",
        "                                     \"also\").replace(\"stadia\", \"stadia_\")))\n",
        "\n",
        "  ### step-wise functions\n",
        "  ### Step 1 - 3: lower case, expand contractions and remove punctuations ###\n",
        "  def process_line(self, line):\n",
        "    \"\"\"Step 1. Convert text to lowercase.\n",
        "    Step 2. expand contractions.\n",
        "    Step 3. Remove punctuations while preserving google internal links (e.g. go/).\n",
        "\n",
        "    Args:\n",
        "      line: (str) a string of words in a sentence.\n",
        "    Returns:\n",
        "      lowercased sentence with no punctuations besides the ones in links.\n",
        "    \"\"\"\n",
        "\n",
        "    if line is not None:\n",
        "      # lower case\n",
        "      temp = line.lower()\n",
        "      # contractions\n",
        "      for k, v in self.contractions.items():\n",
        "        if k in temp:\n",
        "          temp = temp.replace(k, v)\n",
        "      # normalize go links into the same go link\n",
        "      temp = temp.replace(\"go/ link\", \"go link\")\n",
        "      # remove punctuations\n",
        "      temp = temp.translate(str.maketrans(\"\", \"\", self.punc))\n",
        "      return self.combined_processor(temp)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  ### Step 4 & 5: lemmatize with the option for pos-filtering & remove stopwords\n",
        "  ###\n",
        "  def clean(self, doc, lem=True, pos=False):\n",
        "    # Function to remove stop words from sentences & lemmatize verbs and nouns\n",
        "    if doc is not None:\n",
        "      tokenized = word_tokenize(doc)\n",
        "      if pos:\n",
        "        cleaned = [\n",
        "            w[0] for w in nltk.pos_tag(tokenized) if w[1][0] in [\"J\", \"N\", \"R\"]\n",
        "        ]\n",
        "        if lem:\n",
        "          cleaned = [\n",
        "              self.wordnet_lemmatizer.lemmatize(w[0],\n",
        "                                                self.get_wordnet_pos(w[1]))\n",
        "              for w in nltk.pos_tag(tokenized)\n",
        "              if w[1][0] in [\"J\", \"N\", \"R\"]\n",
        "          ]\n",
        "      elif lem:\n",
        "        cleaned = [\n",
        "            self.wordnet_lemmatizer.lemmatize(w[0], self.get_wordnet_pos(w[1]))\n",
        "            for w in nltk.pos_tag(tokenized)\n",
        "        ]\n",
        "      else:\n",
        "        cleaned = tokenized\n",
        "        del tokenized\n",
        "      stop_free = [x for x in cleaned if x not in self.stop_words]\n",
        "      return stop_free\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  # get word positions\n",
        "  def get_wordnet_pos(self, tag):\n",
        "    \"\"\"Convert POS tags from treebank's format.\n",
        "\n",
        "    This represents the categories of words that will actually be affected by\n",
        "    lemmatization.\n",
        "    e.g. a noun like \"wolves\" --> \"wolf\" and a verb like \"sharing\" --> \"share\".\n",
        "\n",
        "    Treebank:\n",
        "      https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "    Wordnet:\n",
        "      http://www.nltk.org/book/ch05.html#a-universal-part-of-speech-tagset\n",
        "\n",
        "    Args:\n",
        "       tag: Treebank POS tag, e.g. \"VBD\" for \"verb past tense\"\n",
        "    Returns:\n",
        "       The corresponding Wordnet tag, e.g. wordnet.VERB. The default in noun.\n",
        "    \"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "      return wordnet.ADJ\n",
        "    elif tag.startswith(\"V\"):\n",
        "      return wordnet.VERB\n",
        "    elif tag.startswith(\"N\"):\n",
        "      return wordnet.NOUN\n",
        "    elif tag.startswith(\"R\"):\n",
        "      return wordnet.ADV\n",
        "    else:\n",
        "      return wordnet.NOUN\n",
        "\n",
        "  # flatten the list\n",
        "  def flatten_list(self, lst):\n",
        "    return [i for t in lst for i in t]\n",
        "\n",
        "  # combines all\n",
        "  def lines_to_topwords(self, intake, n=10, lem=True, pos=False):\n",
        "    \"\"\" process lines and create outputs.\n",
        "      one_line_words: lemmetized and optionally pos-filtered list of words\n",
        "    \"\"\"\n",
        "    one_line_words = one_line.apply(lambda doc: self.flatten_list(\n",
        "        [self.clean(self.process_line(x), lem, pos) for x in sent_tokenize(doc)]))\n",
        "    return one_line_words\n",
        "\n",
        "  ## Other functions\n",
        "  def get_bigrams(self, words_lst, n=2):\n",
        "    # Create n gram dictionary\n",
        "    bigrams = [[g for g in ngrams(w, n)] for w in words_lst]\n",
        "    bigram_dict = Counter(self.flatten_list(bigrams))\n",
        "    return bigrams, bigram_dict\n",
        "\n",
        "  def count_words(self, x, n=5):\n",
        "    return Counter(\n",
        "        [item for sublist in x if sublist is not None\n",
        "         for item in sublist]).most_common(n)\n",
        "\n",
        "  def display_top_n(self, df, col):\n",
        "    n = len(df[col].values[0])\n",
        "    for i in range(n):\n",
        "      df[\"most_popular_{}\".format(\n",
        "          str(i + 1))] = df[col].apply(lambda x: x[i] if len(x) > i else \"\")\n",
        "\n",
        "  def weighted_counter(self, L, c=None):\n",
        "    if c is None:\n",
        "      c = Counter()\n",
        "    for k, v in L:\n",
        "      c.update({k: v})\n",
        "    return c\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W9MOrpt2SjLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF0_BbDlAHFG"
      },
      "outputs": [],
      "source": [
        "TestClass = text_cleaning_helper.text_cleaning_helper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0CvHW7a_Pow"
      },
      "outputs": [],
      "source": [
        "#@title text cleaning\n",
        "cleaned_text = [\" \".join(TestClass.clean(TestClass.process_line(x.decode(\"utf-8\")))) for x in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3HmT_pDdW31"
      },
      "outputs": [],
      "source": [
        "cleaned_text[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21rB9w9rouLK"
      },
      "outputs": [],
      "source": [
        "#@title transform to embeddings\n",
        "embeddings = kona2(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVynPk3hRp-R"
      },
      "outputs": [],
      "source": [
        "plot_similarity(range(3), embeddings[:3], 0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9uBQvNaIlIv"
      },
      "source": [
        "# 3. Cluster the responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV6EqTNZQArL"
      },
      "source": [
        "## 3.1 Build clustering model\n",
        "\n",
        "Since text clusters are rarely of similar sizes and most of the time, we don't know the exact number of clusters there are, this tutorial focuses on clustering methodologies that deals with non-spherical data and do not require pre-determined number of clusters. Usually hierarchical clustering models are more suitable for these needs. One such model that we will cover in this tutorial is the AgglomerativeClustering model in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGp-aFdxbY0Q"
      },
      "outputs": [],
      "source": [
        "def cluster_with_many_ways(embeddings, way = 'complete', nsm = \"cosine\", thres = .7):\n",
        "  \"\"\"now you have the option to safety experiment with other clustering methods\"\"\"\n",
        "\n",
        "  if way in [\"ward\", \"single\", \"average\"]:\n",
        "    nsm = \"euclidean\"\n",
        "    print(\"\"\"The input metric {0} is not compatible with method {1}.\n",
        "    Updated metric to be euclidean.\"\"\".format(nsm, way))\n",
        "  # proprietary of ethelszhang@google.com, default to consine distance\n",
        "  linkage_df = sch.linkage(embeddings, method = way, metric = nsm)\n",
        "  threshold = thres*max(linkage_df[:,2])\n",
        "  ncluster = sum([x[2] > threshold for x in linkage_df]) + 1\n",
        "  # create clusters\n",
        "  hc = AgglomerativeClustering(\n",
        "      n_clusters = ncluster, affinity = nsm, linkage = way)\n",
        "  # save clusters for chart\n",
        "  y_hc = hc.fit_predict(embeddings)\n",
        "  return y_hc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PSqRvGlbXEm"
      },
      "outputs": [],
      "source": [
        "#@title Create dendrogram graph\n",
        "plt.figure(figsize = (18, 5))\n",
        "dendrogram = sch.dendrogram(\n",
        "    sch.linkage(embeddings, method='complete', metric = \"cosine\"),\n",
        "    leaf_rotation=0, above_threshold_color='magenta')\n",
        "plt.hlines(1.25, 0, 1000, color = \"magenta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvI0NcsJdxX5"
      },
      "outputs": [],
      "source": [
        "#@title get clusters\n",
        "y_hc = cluster_with_many_ways(embeddings)\n",
        "\n",
        "# compile output\n",
        "output_df = pd.DataFrame([y_hc, text, intent]).T.rename(\n",
        "    columns = dict(zip(range(3), [\"cluster\", \"text\", \"intent\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6iFUkRYeZeo"
      },
      "outputs": [],
      "source": [
        "# [OPTIONAL] to get lower-level branches\n",
        "output_df[\"sub_cluster\"] = cluster_with_many_ways(embeddings, thres=.6)\n",
        "output_df.sub_cluster.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmUXB1cZe8FA"
      },
      "source": [
        "## 3.2 Explore output [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_cBFg4MnWZl"
      },
      "outputs": [],
      "source": [
        "def upper_tri_indexing(A):\n",
        "    m = A.shape[0]\n",
        "    r,c = np.triu_indices(m,1)\n",
        "    return A[r,c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDdL62EGez-h"
      },
      "outputs": [],
      "source": [
        "# distribution of cluster size\n",
        "plt.hist(output_df.cluster.value_counts());"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LCOjufcHt4e"
      },
      "source": [
        "## 3.3 Summarize the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD4dtqNWfzoP"
      },
      "outputs": [],
      "source": [
        "#@title Get cluster centers (averages)\n",
        "cluster_label_list = output_df.cluster.unique()\n",
        "cluster_idx = output_df.reset_index().groupby(\"cluster\")[\"index\"].unique()\n",
        "\n",
        "cluster_centers = {}\n",
        "cluster_grouped = {}\n",
        "for c in cluster_label_list:\n",
        "  total_embeddings = [embeddings[i,:] for i in cluster_idx[c]]\n",
        "  cluster_ctr = np.mean(total_embeddings, 0)\n",
        "  cluster_centers[c] = cluster_ctr\n",
        "  cluster_grouped[c] = total_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqNRx5R4gOSS"
      },
      "outputs": [],
      "source": [
        "#@title plot cluster correlations\n",
        "plot_similarity(\n",
        "    list(cluster_centers.keys()), list(cluster_centers.values()), 90, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZYcN3xunW2R"
      },
      "outputs": [],
      "source": [
        "#@title histogram of cluster correlations\n",
        "sns.displot(\n",
        "  upper_tri_indexing(\n",
        "    np.inner(list(cluster_centers.values()), list(cluster_centers.values()))\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aYRduSZdy_Z"
      },
      "source": [
        "# 4. Evaluate the clusters of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEhJgsGFp1jk"
      },
      "outputs": [],
      "source": [
        "def top_words(word_lst, n=3):\n",
        "  try:\n",
        "    return \" * \".join([x[0] for x in Counter([w for g in word_lst for w in g]).most_common(n)])\n",
        "  except:\n",
        "    print(\"something wong\")\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8wzy7RoLTrx"
      },
      "outputs": [],
      "source": [
        "output_df[\"norm_text\"] = cleaned_text\n",
        "output_df[\"norm_word_lst\"] = output_df[\"norm_text\"].apply(lambda x: x.split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIv3zt0Uj24d"
      },
      "outputs": [],
      "source": [
        "cluster_df = output_df.groupby(\"cluster\").agg({\n",
        "      \"norm_text\":lambda x: \"\".join(str(x)),\n",
        "      \"intent\":lambda x: \"\".join(str(x)),\n",
        "      \"text\":np.size,\n",
        "      \"norm_word_lst\":top_words}\n",
        "    ).sort_values(\"text\", ascending = False).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-X3BXPqpWu"
      },
      "outputs": [],
      "source": [
        "#@title invetigate how well clusters aligned with intents\n",
        "data_table.DataTable(cluster_df.set_index(\"cluster\"), num_rows_per_page=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtXQ_b3ajzDN"
      },
      "outputs": [],
      "source": [
        "%%vegalite cluster_df\n",
        "\n",
        "mark: bar\n",
        "\n",
        "encoding:\n",
        "  x:\n",
        "    field: cluster\n",
        "    type: quantitative\n",
        "  y:\n",
        "    field: text\n",
        "    type: quantitative\n",
        "  color:\n",
        "    field: norm_word_lst\n",
        "    type: nominal\n",
        "  tooltip:\n",
        "    - field: norm_word_lst\n",
        "    - field: norm_text\n",
        "    - field: intent\n",
        "\n",
        "width: 1000\n",
        "height: 400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7zoR5owiPuv"
      },
      "source": [
        "## Write out"
      ]
    }
  ]
}